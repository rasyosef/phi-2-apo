{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbFC2FuAkjpX"
      },
      "outputs": [],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U --quiet datasets evaluate torch transformers accelerate peft"
      ],
      "metadata": {
        "id": "P1xeoEqvk9FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q git+https://github.com/huggingface/trl.git"
      ],
      "metadata": {
        "id": "lO1BXgo6lIQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load Dataset**"
      ],
      "metadata": {
        "id": "X-HntnZwlN0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "clair_apo = load_dataset(\"ContextualAI/ultrafeedback_clair_32k\")\n",
        "clair_apo"
      ],
      "metadata": {
        "id": "WRduD1KxlKfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  print(clair_apo[\"train\"][i][\"prompt\"])\n",
        "  print(clair_apo[\"train\"][i][\"chosen\"])\n",
        "  print(clair_apo[\"train\"][i][\"rejected\"])\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "s1Erf7D8lRPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load Model**"
      ],
      "metadata": {
        "id": "QWAuDfJAlkw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"rasyosef/phi-2-sft-openhermes-128k-v2-merged\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda\",\n",
        "    # attn_implementation=\"flash_attention_2\"\n",
        "  )"
      ],
      "metadata": {
        "id": "LUIBwtmNlfiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "print(model)"
      ],
      "metadata": {
        "id": "Vga_14Y4loqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\":\"user\", \"content\":\"Who was the last king of Germany?\"}]\n",
        "\n",
        "def chat(messages, max_new_tokens=8):\n",
        "  tokenized_messages = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "  outputs = model.generate(tokenized_messages, max_new_tokens=max_new_tokens)\n",
        "  print(tokenizer.decode(outputs[0]))\n",
        "\n",
        "chat(messages, max_new_tokens=128)"
      ],
      "metadata": {
        "id": "qicCUn2FlrR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Inspect Dataset**"
      ],
      "metadata": {
        "id": "np6BbkDHlvS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clair_apo"
      ],
      "metadata": {
        "id": "7gGm2bvqlt2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(row):\n",
        "    prompt = tokenizer.apply_chat_template(row[\"chosen\"][:-1], tokenize=False, add_generation_prompt=True)\n",
        "    chosen = row[\"chosen\"][-1][\"content\"] + tokenizer.eos_token\n",
        "    rejected = row[\"rejected\"][-1][\"content\"] + tokenizer.eos_token\n",
        "\n",
        "    prompt_length = len(tokenizer.tokenize(prompt))\n",
        "    chosen_length = len(tokenizer.tokenize(chosen))\n",
        "    rejected_length = len(tokenizer.tokenize(rejected))\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"chosen\": chosen,\n",
        "        \"rejected\": rejected,\n",
        "        \"prompt_length\": prompt_length,\n",
        "        \"chosen_length\": chosen_length,\n",
        "        \"rejected_length\": rejected_length,\n",
        "    }\n",
        "\n",
        "clair_apo_processed = clair_apo.map(lambda row: preprocess_dataset(row), num_proc=4)\n",
        "clair_apo_processed"
      ],
      "metadata": {
        "id": "H8WoEouOlzS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lengths Distribution\n",
        "prompt_lengths = sorted(clair_apo_processed[\"train\"][\"prompt_length\"])\n",
        "chosen_lengths = sorted(clair_apo_processed[\"train\"][\"chosen_length\"])\n",
        "rejected_lengths = sorted(clair_apo_processed[\"train\"][\"rejected_length\"])\n",
        "\n",
        "print(\"prompt_lengths:\", prompt_lengths[1024], prompt_lengths[4096], prompt_lengths[8000], prompt_lengths[12000], max(prompt_lengths))\n",
        "print(\"chosen_lengths:\", chosen_lengths[1024], chosen_lengths[4096], chosen_lengths[8000], chosen_lengths[12000], max(chosen_lengths))\n",
        "print(\"rejected_lengths:\", rejected_lengths[1024], rejected_lengths[4096], rejected_lengths[8000], rejected_lengths[12000], max(rejected_lengths))"
      ],
      "metadata": {
        "id": "9aOpiK_hl3kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 512\n",
        "clair_apo_filtered = clair_apo_processed.filter(lambda example: example['prompt_length'] + example['chosen_length'] < MAX_LENGTH and example['prompt_length'] + example['rejected_length'] < MAX_LENGTH)\n",
        "clair_apo_filtered"
      ],
      "metadata": {
        "id": "gLk31eYml6I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "NUM_SAMPLES = 10_000\n",
        "clair_apo_final = clair_apo_filtered['train'].shuffle(seed=42).select(range(NUM_SAMPLES))\n",
        "clair_apo_final = clair_apo_final.train_test_split(test_size=0.02,seed=42)\n",
        "clair_apo_final"
      ],
      "metadata": {
        "id": "tjCck6fHl8gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = clair_apo_final[\"train\"].shuffle().select(range(5))\n",
        "\n",
        "for row in sample:\n",
        "  print(row[\"prompt\"])\n",
        "  print(row[\"chosen\"])\n",
        "  print(row[\"rejected\"])\n",
        "  print(\"\\n-----------------------------------------------------\\n\")"
      ],
      "metadata": {
        "id": "cXe9Z7ZimB2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **APO with TRL**"
      ],
      "metadata": {
        "id": "VQfTwQCQmINR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, cast_mixed_precision_params\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # Target all linear layers\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\", \"fc1\", \"fc2\", \"lm_head\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "cast_mixed_precision_params(model, dtype=torch.float16)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "cp5z6qS5mCpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import DPOConfig, DPOTrainer\n",
        "\n",
        "batch_size = 1\n",
        "gradient_accum_steps = 8\n",
        "epochs = 2\n",
        "\n",
        "new_model_id = \"phi-2-apo\"\n",
        "\n",
        "eval_steps = 250 #len(combined_dpo_final[\"train\"]) // (batch_size * gradient_accum_steps * 8)\n",
        "save_steps = 250\n",
        "logging_steps=eval_steps\n",
        "\n",
        "print(\"Eval Steps:\", eval_steps)\n",
        "print(\"Save Steps:\", save_steps)\n",
        "\n",
        "dpo_config = DPOConfig(\n",
        "  output_dir=new_model_id,\n",
        "  beta=0.1,\n",
        "  max_length=512,\n",
        "  max_prompt_length=512,\n",
        "  per_device_train_batch_size=batch_size,\n",
        "  per_device_eval_batch_size=batch_size,\n",
        "  gradient_accumulation_steps=gradient_accum_steps,\n",
        "  num_train_epochs=epochs,\n",
        "  learning_rate=1e-6,\n",
        "  warmup_steps=0,\n",
        "  lr_scheduler_type=\"cosine\",\n",
        "  remove_unused_columns=False,\n",
        "  fp16=True,\n",
        "  logging_strategy=\"steps\",\n",
        "  logging_steps=logging_steps,\n",
        "  eval_strategy=\"steps\",\n",
        "  eval_steps=eval_steps,\n",
        "  save_strategy=\"steps\",\n",
        "  save_steps=save_steps,\n",
        "  seed=42,\n",
        "  loss_type=\"apo_zero\",\n",
        "  # Optimization Params\n",
        "  # gradient_checkpointing=True,\n",
        "  # hub_token=userdata.get(\"HF_TOKEN\") # Your HuggingFace token\n",
        "  # push_to_hub=True\n",
        ")"
      ],
      "metadata": {
        "id": "DAz1jzC1mO1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = DPOTrainer(\n",
        "    model, # left ref_model null\n",
        "    args=dpo_config,\n",
        "    train_dataset=clair_apo_final[\"train\"],\n",
        "    eval_dataset=clair_apo_final[\"test\"],\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "5aiTMZfKmVGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "9fD8MfG_mXfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(messages):\n",
        "    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(tokenized_chat, max_new_tokens=128) #, stopping_criteria=[\"<|im_end|>\"])\n",
        "    print(tokenizer.decode(outputs[0]))\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": \"What is quantum computing?\"}]\n",
        "chat(messages)"
      ],
      "metadata": {
        "id": "5NoKf7zPmclD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "LQwbumCRmdQ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}